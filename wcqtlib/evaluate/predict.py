import logging
import numpy as np
import pandas
import progressbar

import wcqtlib.common.labels as labels
instrument_map = labels.InstrumentClassMap()

logger = logging.getLogger(__name__)


def predict_one(dfrecord, model, slicer_fx, t_len):
    """Return an evaluation object/dict after evaluating
    a single model using a loaded model.

    This method runs the model.predict over every set of
    frames generated by slicer_fx, and returns
    the class with the maximum

    Parameters
    ----------
    dfrecord : pandas.DataFrame
        pandas.Series containing the record to evaluate.

    model : models.NetworkManager

    slicer_fx : function
        Function that extracts featuers fr eaach frame
        from the feature file.

    t_len : int

    Returns
    ------
    results : pandas.Series
        All the results stored as a pandas.Series
    """
    # Get predictions for every timestep in the file.
    results = []
    losses = []
    accs = []
    target = instrument_map.get_index(dfrecord["instrument"])

    for frames in slicer_fx(dfrecord, t_len=t_len,
                            shuffle=False, auto_restart=False):
        results += [model.predict(frames)]
        loss, acc = model.evaluate(frames)
        losses += [loss]
        accs += [acc]
    if len(results) and len(losses) and len(accs):
        results = np.concatenate(results)
        mean_loss = np.mean(losses)
        mean_acc = np.mean(accs)

        class_predictions = results.argmax(axis=1)

        # calculate the maximum likelihood class - the class with the highest
        #  predicted probability across all frames.
        max_likelihood_class = results.max(axis=0).argmax()

        # Also calculate the highest voted frame.
        vote_class = np.asarray(np.bincount(class_predictions).argmax(),
                                dtype=np.int)

        # Return both of these as a dataframe.
        return pandas.Series(
            data=[mean_loss, mean_acc, max_likelihood_class,
                  vote_class, target],
            index=["mean_loss", "mean_acc", "max_likelihood",
                   "vote", "target"],
            name=dfrecord.name)
    else:
        return pandas.Series(
            index=["mean_loss", "mean_acc", "max_likelihood",
                   "vote", "target"],
            name=dfrecord.name)


def predict_many(test_df, model, slicer_fx, t_len, show_progress=False):
    """Run evaluation on the files in a dataframe.

    Parameters
    ----------
    test_df : pandas.DataFrame
        DataFrame pointing to the features files and targets to evaluate.

    model : models.NetworkManager

    slicer_fx : function
        Function that extracts featuers fr eaach frame
        from the feature file.

    t_len : int

    Returns
    -------
    results_df : pandas.DataFrame
        DataFrame containing the results from for each file,
        where the index of the original file is maintained, but
        the dataframe now contains the columns:
            * max_likelihood
            * vote
            * target
    """
    results = []
    if show_progress:
        i = 0
        progress = progressbar.ProgressBar(max_value=len(test_df))

    try:
        for index, row in test_df.iterrows():
            results += [predict_one(row, model, slicer_fx, t_len)]

            if show_progress:
                progress.update(i)
                i += 1
    except KeyboardInterrupt:
        logger.error("Evaluation process interrupted; {} of {} evaluated."
                     .format(len(results), len(test_df)))
        logger.error("Recommend you start this process over to evaluate "
                     "them all.")

    return pandas.DataFrame(results)
